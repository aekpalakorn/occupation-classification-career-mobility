{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "096bc7d8-defa-46f9-b831-69be3e5a8b16",
   "metadata": {},
   "source": [
    "# OEWS Code Standardization\n",
    "\n",
    "This notebook documents the end-to-end process to standardize occupational codes (OCC_CODE) in BLS's State Occupational Employment and Wage Estimates (OEWS) data to 6-digit SOC 2019 codes. The standardized OEWS table was used for enriching the Career229K dataset with occupational wage information as described in the paper \"Leveraging Large Language Models for Career Mobility Analysis: A Study of Gender, Race, and Job Change Using U.S. Online Resume Profiles.\"\n",
    "\n",
    "**Important Note on Reproducibility**\n",
    "\n",
    "* This pipeline relies on public BLS OEWS data and O*NET SOC crosswalks.\n",
    "* The codebase reproduces the data processing steps described in the paper.\n",
    "* Minor divergences from the wage tables used in the paper exist (e.g., due to code mapping and aggregation differences), but downstream effects are minimal.\n",
    "* The file names used in the code are placeholders. To run this notebook, users must substitute them with their own files.\n",
    "\n",
    "## Overview of Pipeline\n",
    "\n",
    "The pipeline has four main stages following the steps used in the paper :\n",
    "\n",
    "### 1. Generate SOC Code Mapping Table\n",
    "\n",
    "Construct a lookup table mapping all legacy occupational codes (``OCC_CODE``) in the OEWS tables from 1999 - 2022 to O*NET-SOC 2019 6-digit codes, using multiple BLS crosswalk files. The mapping accounts for SOC system updates and code splits and merges over time.\n",
    "\n",
    "### 2. Standardize Occupational Codes in OEWS Data\n",
    "\n",
    "Apply the mapping table to standardize all ``OCC_CODE`` entries in the OEWS tables. If multiple old codes map to the same SOC 2019 code, wage values are aggregated by mean within each group.\n",
    "\n",
    "### 3. Global Imputation of Standardized OEWS Data\n",
    "\n",
    "Remaining missing wage values within each `(OCC_CODE, AREA_TITLE)` group are filled using linear interpolation over time.\n",
    "\n",
    "## Input Files\n",
    "\n",
    "### 1. OEWS CSV File\n",
    "\n",
    "An OEWS wage table compiled from the original annual OEWS data files. This file contains the raw occupational wage information for all areas and occupations from 1997 - 2002 prior to code standardization. Download it [here](https://drive.google.com/file/d/1bifxmMD-IldLH2OwweHg1s18w9aYwTnv/view?usp=drive_link).\n",
    "\n",
    "Required columns:\n",
    "\n",
    "| Column       | Description                                                                                                             |\n",
    "| ------------ | ----------------------------------------------------------------------------------------------------------------------- |\n",
    "| `year`       | Survey year (1997–2022, though only 1999+ is used)                                                                      |\n",
    "| `AREA_TITLE` | Geographic area (e.g., “California”)                                                                                    |\n",
    "| `OCC_CODE`   | Original SOC code (maybe legacy or SOC 2019)                                                                            |\n",
    "| `OCC_TITLE`  | Occupation title                                                                                                        |\n",
    "| `O_GROUP`    | BLS occupation group (filtered to `\"detailed\"`)                                                                         |\n",
    "| Wage fields  | `A_MEAN`, `H_MEAN`, `A_MEDIAN`, `H_MEDIAN`, and so on.                                                                  |\n",
    "\n",
    "### 2. SOC Crosswalk CSV Files\n",
    "\n",
    "These are used to construct the mapping from older SOC versions to SOC 2019. Downloadable from [the O*NET Taxonomy website)](https://www.onetcenter.org/taxonomy.html).\n",
    "\n",
    "### 3. SOC Mapping CSV File\n",
    "\n",
    "A complete SOC crosswalk table that maps all 8-digit legacy SOC codes to 8-digit SOC 2019 codes, using multiple O*NET crosswalk files, created in Stetp 1. The mapping table is constructed via depth-first search through the crosswalk graph to trace all possible paths from legacy codes to their SOC 2019 equivalents. For convenience, a pre-generated mapping file is availabel at `/data/soc_non2019_to_2019_mapping.csv`.\n",
    "\n",
    "**Required columns:**\n",
    "\n",
    "| Column                   | Description                                                                                     |\n",
    "| ------------------------ | ----------------------------------------------------------------------------------------------- |\n",
    "| `original_code`          | Original SOC code as it appeared in legacy OEWS data                                            |\n",
    "| `normalized_code`        | Canonicalized SOC code (e.g., `151030` → `15-1030.00`) used for consistent merging              |\n",
    "| `mapped_soc2019_code`    | Final 6-digit SOC 2019 code reached through crosswalk traversal                                 |\n",
    "| `mapped_soc2019_title`   | Official SOC 2019 occupation title corresponding to `mapped_soc2019_code`                       |\n",
    "| `path_length`            | Number of crosswalk steps required to reach SOC 2019 (e.g., 0 if already SOC 2019, 3 if legacy) |\n",
    "| `first_detected_version` | Earliest SOC taxonomy version in which the original code appeared                               |\n",
    "| `last_detected_version`  | Last SOC taxonomy version encountered along the mapping path                                    |\n",
    "\n",
    "### 4. O*NET-SOC 2018 CSV File\n",
    "\n",
    "Used for mapping SOC codes to their corresponding official occupation titles (``OCC_TITLE``) after standardization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70cc663-aca4-41e9-8227-e44aead271bb",
   "metadata": {},
   "source": [
    "# Generate SOC Code Mapping Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80390d7-e0a9-4346-91de-3b838ecbdef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "CROSSWALK_FILES = {\n",
    "    \"2000_2006\": \"../data/2000_to_2006_Crosswalk.csv\",\n",
    "    \"2006_2009\": \"../data/2006_to_2009_Crosswalk.csv\",\n",
    "    \"2009_2010\": \"../data/2009_to_2010_Crosswalk.csv\",\n",
    "    \"2010_2019\": \"../data/2010_to_2019_Crosswalk.csv\",\n",
    "}\n",
    "SOC2019_PATH = \"../data/onet-soc_2019.csv\"\n",
    "OUT_MAPPING_CSV = \"../data/soc_non2019_to_2019_mapping.csv\"\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "def normalize_soc_code(code):\n",
    "    if pd.isna(code):\n",
    "        return None\n",
    "    s = str(code).strip()\n",
    "    if s.lower() in {\"\", \"na\", \"nan\", \"n/a\", \"none\"}:\n",
    "        return None\n",
    "\n",
    "    # Remove quotes\n",
    "    s = s.strip('\"').strip(\"'\")\n",
    "\n",
    "    # Remove non-digit/non-dot characters except dash\n",
    "    s = re.sub(r'[^0-9\\.-]', '', s)\n",
    "\n",
    "    # Split decimal if exists\n",
    "    if '.' in s:\n",
    "        main, suffix = s.split('.')\n",
    "        suffix = suffix.ljust(2, '0')  # pad to 2 digits\n",
    "    else:\n",
    "        main = s\n",
    "        suffix = '00'\n",
    "\n",
    "    # Insert dash in main if needed\n",
    "    main_digits = re.sub(r'\\D', '', main)\n",
    "    if len(main_digits) != 6:\n",
    "        return None  # invalid\n",
    "    main_formatted = main_digits[:2] + '-' + main_digits[2:]\n",
    "\n",
    "    return f\"{main_formatted}.{suffix}\"\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Load SOC 2019 taxonomy\n",
    "# -----------------------------\n",
    "soc2019_df = pd.read_csv(SOC2019_PATH, dtype=str)\n",
    "soc2019_df['soc_norm'] = soc2019_df['code'].apply(normalize_soc_code)\n",
    "soc2019_codes = set(soc2019_df['soc_norm'].dropna())\n",
    "soc2019_title_map = dict(zip(soc2019_df['soc_norm'], soc2019_df['title']))\n",
    "print(f\"Loaded {len(soc2019_codes)} SOC 2019 codes\")\n",
    "\n",
    "# -----------------------------\n",
    "# Build crosswalk adjacency with version tracking\n",
    "# -----------------------------\n",
    "version_map = {\n",
    "    \"2000_2006\": (\"O*NET-SOC 2000 Code\", \"O*NET-SOC 2006 Code\", \"O*NET-SOC 2006 Title\", \"2000\"),\n",
    "    \"2006_2009\": (\"O*NET-SOC 2006 Code\", \"O*NET-SOC 2009 Code\", \"O*NET-SOC 2009 Title\", \"2006\"),\n",
    "    \"2009_2010\": (\"O*NET-SOC 2009 Code\", \"O*NET-SOC 2010 Code\", \"O*NET-SOC 2010 Title\", \"2009\"),\n",
    "    \"2010_2019\": (\"O*NET-SOC 2010 Code\", \"O*NET-SOC 2019 Code\", \"O*NET-SOC 2019 Title\", \"2010\"),\n",
    "}\n",
    "\n",
    "code_to_next = dict()\n",
    "code_to_version = dict()  # node -> version\n",
    "\n",
    "for ver, path in CROSSWALK_FILES.items():\n",
    "    df = pd.read_csv(path, dtype=str)\n",
    "    src_col, tgt_col, tgt_title_col, ver_label = version_map[ver]\n",
    "    for _, row in df.iterrows():\n",
    "        src_norm = normalize_soc_code(row[src_col])\n",
    "        tgt_norm = normalize_soc_code(row[tgt_col])\n",
    "        tgt_title = row[tgt_title_col]\n",
    "        if src_norm and tgt_norm:\n",
    "            code_to_next.setdefault(src_norm, []).append((tgt_norm, tgt_title))\n",
    "            code_to_version[src_norm] = ver_label\n",
    "            code_to_version[tgt_norm] = ver_label\n",
    "\n",
    "print(f\"Built stepwise crosswalk with {len(code_to_next)} source codes\")\n",
    "\n",
    "# -----------------------------\n",
    "# DFS to collect all paths to SOC 2019\n",
    "# -----------------------------\n",
    "def dfs_collect_2019(code, first_ver=None, visited=None):\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "    if code in visited:\n",
    "        return []\n",
    "    visited.add(code)\n",
    "\n",
    "    # Stop only if truly SOC 2019\n",
    "    if code in soc2019_codes:\n",
    "        return [(code, 0, first_ver or code_to_version.get(code, \"unknown\"),\n",
    "                 code_to_version.get(code, \"unknown\"))]\n",
    "\n",
    "    results = []\n",
    "    for next_code, _ in code_to_next.get(code, []):\n",
    "        next_ver = code_to_version.get(next_code, \"unknown\")\n",
    "        new_first_ver = first_ver or code_to_version.get(code, \"unknown\")\n",
    "        sub_paths = dfs_collect_2019(next_code, new_first_ver, visited.copy())\n",
    "        for fc, path_len, first_detected, last_detected in sub_paths:\n",
    "            results.append((fc, path_len + 1, first_detected, next_ver))\n",
    "    return results\n",
    "\n",
    "# -----------------------------\n",
    "# Map all non-2019 codes\n",
    "# -----------------------------\n",
    "all_old_codes = set(code_to_next.keys()) - soc2019_codes\n",
    "mapping_rows = []\n",
    "unmapped_codes = []\n",
    "\n",
    "for i, orig_code in enumerate(sorted(all_old_codes)):\n",
    "    orig_norm = normalize_soc_code(orig_code)\n",
    "    final_paths = dfs_collect_2019(orig_norm)\n",
    "    if final_paths:\n",
    "        for fc, path_len, first_ver, last_ver in final_paths:\n",
    "            mapping_rows.append({\n",
    "                \"original_code\": orig_code,\n",
    "                \"normalized_code\": orig_norm,\n",
    "                \"mapped_soc2019_code\": fc,\n",
    "                \"mapped_soc2019_title\": soc2019_title_map.get(fc),\n",
    "                \"path_length\": path_len,\n",
    "                \"first_detected_version\": first_ver,\n",
    "                \"last_detected_version\": last_ver\n",
    "            })\n",
    "    else:\n",
    "        unmapped_codes.append(orig_code)\n",
    "\n",
    "# -----------------------------\n",
    "# Save mapping\n",
    "# -----------------------------\n",
    "mapping_df = pd.DataFrame(mapping_rows)\n",
    "mapping_df.to_csv(OUT_MAPPING_CSV, index=False)\n",
    "print(f\"\\nStepwise crosswalk mapping saved to {OUT_MAPPING_CSV}\")\n",
    "print(f\"Total mapped codes: {len(mapping_df)}\")\n",
    "print(f\"Total unmapped codes: {len(unmapped_codes)}\")\n",
    "if unmapped_codes:\n",
    "    print(\"Some codes could not be mapped to 2019:\")\n",
    "    print(\", \".join(unmapped_codes[:20]) + (\"...\" if len(unmapped_codes) > 20 else \"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fff4ca-ed79-416e-8a4f-1e1b56c81432",
   "metadata": {},
   "source": [
    "# Standardize Occupational Codes in OEWS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb7aed0-6614-4047-b8c0-a9518200f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def standardize_wage_soc2019(wage_df, mapping_df, soc2018_titles_df, verbose=True):\n",
    "    \"\"\"\n",
    "    Standardize BLS wage data to SOC 2019 codes using stepwise crosswalk mapping.\n",
    "    - Aggregates wage rows by mean for each (year, AREA_TITLE, OCC_CODE, OCC_TITLE).\n",
    "    - Flags rows as 'is_aggregate' if multiple legacy codes are collapsed.\n",
    "    - Returns aggregated standardized DataFrame only.\n",
    "    \"\"\"\n",
    "    start_total = time.time()\n",
    "\n",
    "    # --- 1. Filter wage data ---\n",
    "    start = time.time()\n",
    "    wage_df = wage_df[wage_df['year'].astype(int) >= 1999].copy()\n",
    "    wage_df = wage_df[wage_df['O_GROUP'].isna() | (wage_df['O_GROUP'] == 'detailed')].copy()\n",
    "    wage_df['OLD_OCC_CODE'] = wage_df['OCC_CODE']\n",
    "    wage_df['OLD_OCC_TITLE'] = wage_df['OCC_TITLE']\n",
    "    wage_df = wage_df.drop(columns=['O_GROUP'])\n",
    "    wage_df['OCC_CODE_TRUNC'] = wage_df['OLD_OCC_CODE'].str[:7]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Filtering and truncating done in {time.time() - start:.2f} seconds\")\n",
    "\n",
    "    # --- 2. Prepare mapping ---\n",
    "    mapping_df = mapping_df.copy()\n",
    "    mapping_df['original_code_trunc'] = mapping_df['original_code'].str[:7]\n",
    "    mapping_df['mapped_soc2019_code_trunc'] = mapping_df['mapped_soc2019_code'].str[:7]\n",
    "\n",
    "    # --- 3. Merge wage data with mapping ---\n",
    "    start = time.time()\n",
    "    merged = wage_df.merge(\n",
    "        mapping_df[['original_code_trunc', 'mapped_soc2019_code_trunc']],\n",
    "        left_on='OCC_CODE_TRUNC',\n",
    "        right_on='original_code_trunc',\n",
    "        how='left'\n",
    "    )\n",
    "    merged['OCC_CODE'] = merged['mapped_soc2019_code_trunc'].fillna(merged['OCC_CODE_TRUNC'])\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Merging wage with mapping done in {time.time() - start:.2f} seconds\")\n",
    "        print(f\"# unique OCC_CODE after merging: {merged['OCC_CODE'].nunique()}\")\n",
    "\n",
    "    # --- 4. Map SOC2018 titles ---\n",
    "    start = time.time()\n",
    "    soc2018_title_map = dict(zip(soc2018_titles_df['OCC_CODE'], soc2018_titles_df['OCC_TITLE_SOC2018']))\n",
    "    merged['OCC_TITLE'] = merged['OCC_CODE'].map(soc2018_title_map)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Mapping SOC2018 titles done in {time.time() - start:.2f} seconds\")\n",
    "\n",
    "    # --- 5. Mean aggregation ---\n",
    "    start = time.time()\n",
    "    wage_cols = [\n",
    "        'TOT_EMP', 'H_MEAN', 'A_MEAN', 'H_MEDIAN', 'A_MEDIAN',\n",
    "        'H_PCT10', 'A_PCT10', 'H_PCT25', 'A_PCT25', 'H_PCT90', 'A_PCT90'\n",
    "    ]\n",
    "\n",
    "    # Ensure numeric\n",
    "    for col in wage_cols:\n",
    "        merged[col] = pd.to_numeric(merged[col], errors='coerce')\n",
    "\n",
    "    # Mean aggregation\n",
    "    aggregated = merged.groupby(\n",
    "        ['year', 'AREA_TITLE', 'OCC_CODE', 'OCC_TITLE'],\n",
    "        as_index=False\n",
    "    )[wage_cols].mean()\n",
    "\n",
    "    # Compute group sizes to flag aggregates\n",
    "    group_sizes = merged.groupby(\n",
    "        ['year', 'AREA_TITLE', 'OCC_CODE', 'OCC_TITLE']\n",
    "    ).size().reset_index(name='group_size')\n",
    "    aggregated = aggregated.merge(group_sizes, on=['year', 'AREA_TITLE', 'OCC_CODE', 'OCC_TITLE'])\n",
    "    aggregated['is_aggregate'] = aggregated['group_size'] > 1\n",
    "    aggregated = aggregated.drop(columns=['group_size'])\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Mean aggregation and aggregate flag done in {time.time() - start:.2f} seconds\")\n",
    "\n",
    "    merged = None  # free memory\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Total pipeline runtime: {time.time() - start_total:.2f} seconds\")\n",
    "\n",
    "    return aggregated\n",
    "\n",
    "# -----------------------------\n",
    "# Run standardization\n",
    "# -----------------------------\n",
    "wage_df = pd.read_csv(f\"../data/wage_1997_2022.csv\", low_memory=False)\n",
    "unique_file = \"../data/wage_1999_2022_soc2019_unique.csv.gz\"\n",
    "unique_df = standardize_wage_soc2019(wage_df, mapping_df, soc2018_titles_df, verbose=True)\n",
    "unique_df.to_csv(unique_file, index=False, compression='gzip')\n",
    "print(f\"Final unique wage table saved: {unique_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a376a849-2157-4760-a356-c5bf07c4e1fb",
   "metadata": {},
   "source": [
    "# Global Imputation of Standardized OEWS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7e6eaf-9296-4379-9eab-ad033be8dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def impute_wage_timeseries(df, progress_every=5000):\n",
    "    \"\"\"\n",
    "    Impute all wage columns via linear interpolation between years within each (OCC_CODE, AREA_TITLE) group.\n",
    "    - Only interpolates, no forward/backward fill.\n",
    "    - Includes progress printout for large datasets.\n",
    "    \"\"\"\n",
    "    wage_cols = ['H_MEAN', 'A_MEAN', 'H_MEDIAN', 'A_MEDIAN',\n",
    "                 'H_PCT10', 'A_PCT10', 'H_PCT25', 'A_PCT25',\n",
    "                 'H_PCT90', 'A_PCT90']\n",
    "\n",
    "    # Ensure numeric\n",
    "    df[wage_cols] = df[wage_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Sort for interpolation\n",
    "    df = df.sort_values(['OCC_CODE', 'AREA_TITLE', 'year'])\n",
    "\n",
    "    grouped = df.groupby(['OCC_CODE', 'AREA_TITLE'], group_keys=False)\n",
    "    total_groups = len(grouped)\n",
    "    print(f\"Starting imputation for {total_groups:,} groups...\")\n",
    "\n",
    "    result = []\n",
    "    for i, (_, group) in enumerate(grouped):\n",
    "        # Interpolate within group\n",
    "        group[wage_cols] = group[wage_cols].interpolate(method='linear', axis=0)\n",
    "\n",
    "        if (i + 1) % progress_every == 0:\n",
    "            print(f\"  → Processed {i + 1:,}/{total_groups:,} groups...\")\n",
    "\n",
    "        result.append(group)\n",
    "\n",
    "    df_imputed = pd.concat(result)\n",
    "    print(\"Imputation complete.\")\n",
    "    return df_imputed\n",
    "\n",
    "# Load and impute\n",
    "wage_std_df = pd.read_csv(\"../data/wage_1999_2022_soc2019_unique.csv.gz\")\n",
    "start = time.time()\n",
    "wage_interp_df = impute_wage_timeseries(wage_std_df)\n",
    "end = time.time()\n",
    "print(f\"Done in {end-start:.2f} sec\")\n",
    "\n",
    "# Save\n",
    "wage_interp_df.to_csv(\n",
    "    \"../data/wage_interpolated_1999_2022_soc2019_unique.csv.gz\",\n",
    "    index=False, compression='gzip'\n",
    ")\n",
    "print(\"Imputation complete and saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
